# Reinforcement Learning Fine-Tuning (RLFT)
RL based Fine Tuning of GPT Models for my own understanding and sharing it with others as a part of Giving-back series.


## 1. Building Nano-GPT
This is direct copy from the tutorials of Andrej Karpathy on building GPT from scratch. Please refer repo [here](https://github.com/karpathy/ng-video-lecture)

Summary:
1. Character and token embeddings: represent character/token as a vector which is used throughout the model instead of working in one-hot space
2. Bigram Model: next token is only dependent on current token i.e. we use $p(x_{t+1} | x_t)$
3. Attention Block intuition: to get the context, change the current embedding to average of embedding $\sum x_{\leq t}$; attention is just weighted combination.
4. Nano-gpt model

## 2. Reinforcement Learning Fine-Tuning of Language Models
 
### Introduction
Reinforcement Learning Fine-Tuning (RLFT) has become a central paradigm for aligning large language models (LLMs) with human preferences. Unlike classical supervised training, RLFT incorporates feedback (human or automated) to optimize model outputs towards desirable behaviors. This chapter introduces the three major RLFT approaches:

- Reinforcement Learning from Human Feedback (RLHF, typically using PPO),
- Direct Preference Optimization (DPO),
- Group Relative Policy Optimization (GRPO).

Each method is introduced along with its mathematical formulation, nuances, and clarifications of common misconceptions.

---

### Reinforcement Learning from Human Feedback (RLHF)

#### Problem Setup
Let $x$ denote a prompt, and $y = (y_1,\dots,y_T)$ a completion generated by an autoregressive policy $\pi_\theta$:

$$
\pi_\theta(y \mid x) = \prod_{t=1}^T \pi_\theta(y_t \mid x, y_{<t}).
$$

A reward model $r_\phi(x,y)$ is trained from preference data to provide a scalar score. The goal is to update $\pi_\theta$ such that preferred completions are more likely.

#### Contextual Bandit View
Unlike standard RL environments, RLHF is essentially a **one-step episodic MDP** (contextual bandit):

- State: prompt $x$,
- Action: full completion $y$,
- Reward: $R = r_\phi(x,y)$,
- Episode terminates after reward is given.

Even though the reward arrives only at the end, gradients propagate tokenwise:

$$
\nabla_\theta J(\theta) 
= \mathbb{E}_{x,y \sim \pi_\theta}\left[ r(x,y) \sum_{t=1}^T \nabla_\theta \log \pi_\theta(y_t \mid x,y_{<t}) \right].
$$

This is mathematically equivalent to sequence-level REINFORCE but is implemented per-token in practice to support PPO clipping and per-token advantages.

#### PPO Optimization
Given samples from $\pi_{\text{old}}$, define

$$
\rho_t(\theta) = \frac{\pi_\theta(y_t \mid s_t)}{\pi_{\text{old}}(y_t \mid s_t)}.
$$

The clipped PPO objective is

$$
L_{\text{PPO}}(\theta) = 
\mathbb{E}_t \left[
\min\!\left( \rho_t(\theta)\,\hat A_t,\;\text{clip}(\rho_t(\theta), 1-\epsilon, 1+\epsilon)\,\hat A_t \right)
\right].
$$

Here $\hat A_t$ is an advantage estimate. In practice, with outcome supervision, the same reward $R$ is broadcast to all tokens, possibly normalized with a batch baseline.

#### KL Regularization
A KL penalty anchors the fine-tuned policy to a reference model $\pi_0$ (usually the SFT model):

$$
\widehat{\mathrm{KL}} = \mathbb{E}_t\left[ \log \pi_\theta(y_t \mid s_t) - \log \pi_0(y_t \mid s_t) \right].
$$

Final loss:

$$
\mathcal{L}(\theta) = - L_{\text{PPO}}(\theta) + \beta \,\widehat{\mathrm{KL}}.
$$

#### Clarifications
- **One-step vs multi-step:** Although the environment is one-step, tokenwise updates are necessary because PPO requires per-token log-prob ratios.  
- **Advantage vs baseline:** Advantage $\hat A_t = Q(s_t,a_t)-V(s_t)$ emerges as REINFORCE with a value-function baseline; subtracting $V(s)$ reduces variance.  
- **KL meaning:** Both PPO and RLHF use KL *on action distributions*, not on parameters.  

---

### Direct Preference Optimization (DPO)

#### From Preferences to Policy
Instead of learning a reward model, DPO directly optimizes the policy from preference pairs $(x,y_w,y_l)$ with $y_w \succ y_l$.

The Bradley–Terry model gives:

$$
P(y_w \succ y_l \mid x) = \sigma\!\Big(\tfrac{1}{\beta}(r(x,y_w)-r(x,y_l))\Big).
$$

#### Reward–Policy Connection
From maximum entropy RL, the optimal policy satisfies

$$
\pi^*(y \mid x) \propto \pi_0(y \mid x) \exp\!\Big(\tfrac{1}{\beta} r(x,y)\Big).
$$

Rearranging:

$$
r(x,y) = \beta \Big(\log \pi^*(y \mid x) - \log \pi_0(y \mid x)\Big) + \text{const}.
$$

Substituting into Bradley–Terry yields the DPO loss:

$$
\mathcal{L}_{\text{DPO}}(\theta) = - \mathbb{E}_{(x,y_w,y_l)} \log \sigma\!\Big(\beta\big[
\log \tfrac{\pi_\theta(y_w \mid x)}{\pi_0(y_w \mid x)}
- \log \tfrac{\pi_\theta(y_l \mid x)}{\pi_0(y_l \mid x)} \big]\Big).
$$

#### Clarifications
- **Not RL in practice:** DPO is implemented as supervised logistic regression on preference pairs. No reward model or critic is trained.  
- **Why log-ratio?** The log-ratio $\log \pi_\theta - \log \pi_0$ measures preference-aligned deviation from the reference; this keeps optimization stable.  
- **Equivalence:** DPO can be derived as a direct surrogate of PPO-RLHF under the reward–policy duality.  

---

### Group Relative Policy Optimization (GRPO)

#### Motivation
GRPO seeks to retain PPO-style RL optimization without requiring a critic. It replaces the value-function baseline with **group-relative normalization**.

#### Group Relative Advantage
For a prompt $x$, generate $G$ completions with rewards $\{r_i\}$. Normalize within the group:

$$
\tilde r_i = \frac{r_i - \mu}{\sigma}, \quad 
\mu = \tfrac{1}{G}\sum_{j=1}^G r_j, \quad 
\sigma = \sqrt{\tfrac{1}{G}\sum_{j=1}^G (r_j-\mu)^2}.
$$

Then assign per-token advantages:

$$
\hat A_{i,t} = \tilde r_i \qquad \forall t \in y_i,
$$

or cumulative step-level sums in process supervision.

#### GRPO Objective
The PPO objective with group-relative advantages is:

$$
J_{\text{GRPO}}(\theta) =
\mathbb{E}\Big[\min(\rho_{i,t}\hat A_{i,t},\;\text{clip}(\rho_{i,t},1-\epsilon,1+\epsilon)\hat A_{i,t})\Big]
- \beta\,D_{\mathrm{KL}}(\pi_\theta \,\|\, \pi_{\text{ref}}).
$$

#### Clarifications
- **No critic:** Group mean replaces $V(s)$ as a baseline; variance reduction is achieved without training a value network.  
- **Relation to PPO:** Still an RL-style policy gradient with importance ratios and clipping, unlike DPO.  
- **Interpretation:** Each completion is judged relative to peers; this reflects comparative feedback rather than absolute reward.  

---

## Summary
- **RLHF (PPO):** Classical RL-style update with a reward model and critic; environment is one-step but updates are tokenwise.  
- **DPO:** Removes reward model and critic; reduces to supervised preference optimization with a log-ratio loss.  
- **GRPO:** Retains PPO mechanics but eliminates critic via group-relative normalization of rewards.  

All three methods are mathematically connected, but differ in practice: RLHF is heavy RL, DPO is supervised contrastive learning, and GRPO is a middle ground that keeps PPO structure while simplifying baseline estimation.
