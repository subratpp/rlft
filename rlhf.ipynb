{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9994303d",
   "metadata": {},
   "source": [
    "# RLHF with Toy Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38e96c3",
   "metadata": {},
   "source": [
    "The RLHF / RLFT procedure can be illustrated in a very compact form:\n",
    "\n",
    "1. **Policy** (language model as policy):\n",
    "   \n",
    "   $$\n",
    "   \\pi_\\theta(y \\mid x) = \\prod_{t=1}^T \\pi_\\theta(y_t \\mid x, y_{<t})\n",
    "   $$\n",
    "\n",
    "2. **Reward model** assigns a scalar score to the prompt–completion pair:\n",
    "   \n",
    "   $$\n",
    "   R = r_\\phi(x,y) \\in \\mathbb{R}\n",
    "   $$\n",
    "\n",
    "3. **Baseline and advantage**  \n",
    "   To reduce variance we normalize rewards in the batch, giving a simple advantage signal:\n",
    "   \n",
    "   $$\n",
    "   \\bar{R} = \\frac{1}{N}\\sum_{i=1}^N R_i,\n",
    "   \\qquad\n",
    "   \\hat{A}_t = R - \\bar{R} \\quad (\\text{broadcast to tokens})\n",
    "   $$\n",
    "\n",
    "4. **Log-probabilities and importance ratio**  \n",
    "   We compare the new policy to the old one that generated the samples:\n",
    "   \n",
    "   $$\n",
    "   \\ell^{\\text{old}}_t = \\log \\pi_{\\text{old}}(a_t \\mid s_t),\n",
    "   \\qquad\n",
    "   \\ell_t = \\log \\pi_\\theta(a_t \\mid s_t)\n",
    "   $$\n",
    "   \n",
    "   $$\n",
    "   \\rho_t = \\exp(\\ell_t - \\ell^{\\text{old}}_t)\n",
    "   $$\n",
    "\n",
    "5. **PPO surrogate objective**  \n",
    "   Encourages improvement while clipping updates to avoid divergence:\n",
    "   \n",
    "   $$\n",
    "   L_{\\text{PPO}}(\\theta) = \\mathbb{E}_t \\Big[\n",
    "   \\min\\big(\\rho_t \\hat{A}_t,\\;\n",
    "   \\text{clip}(\\rho_t, 1-\\epsilon, 1+\\epsilon)\\,\\hat{A}_t \\big)\n",
    "   \\Big]\n",
    "   $$\n",
    "\n",
    "6. **KL-to-reference penalty**  \n",
    "   Keeps the policy close to the supervised (SFT) reference model:\n",
    "   \n",
    "   $$\n",
    "   \\widehat{\\mathrm{KL}} = \\mathbb{E}_t\\big[\\ell_t - \\ell^{\\text{ref}}_t\\big],\n",
    "   \\qquad\n",
    "   \\ell^{\\text{ref}}_t = \\log \\pi_{\\text{ref}}(a_t \\mid s_t)\n",
    "   $$\n",
    "\n",
    "7. **Final loss** (to minimize):\n",
    "   \n",
    "   $$\n",
    "   \\mathcal{L}(\\theta) = - L_{\\text{PPO}}(\\theta)\\;+\\;\\beta \\,\\widehat{\\mathrm{KL}}\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**  \n",
    "- The policy generates completions.  \n",
    "- The reward model (or preference signal) gives a scalar reward.  \n",
    "- We normalize rewards to compute a baseline-adjusted advantage.  \n",
    "- PPO surrogate ensures stable updates.  \n",
    "- KL regularization prevents the model from drifting too far from the supervised reference.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd89712b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy_rlft_bigram_ppo.py\n",
    "import math, random, copy\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(0); random.seed(0)\n",
    "\n",
    "# -----------------------------\n",
    "# Tiny tokenizer (char-level)\n",
    "# -----------------------------\n",
    "PROMPTS = [\n",
    "    \"Q: 2+3=\\nA:\", \"Q: 1+4=\\nA:\", \"Q: 4+4=\\nA:\", \"Q: 3+2=\\nA:\",\n",
    "    \"Q: 5+0=\\nA:\", \"Q: 6-1=\\nA:\", \"Q: 7-2=\\nA:\", \"Q: 9-4=\\nA:\"\n",
    "]\n",
    "# Gold short answers for an \"auto-judge\" (to create preferences)\n",
    "GOLD = {\n",
    "    \"Q: 2+3=\\nA:\": \"5\", \"Q: 1+4=\\nA:\": \"5\", \"Q: 4+4=\\nA:\": \"8\",\n",
    "    \"Q: 3+2=\\nA:\": \"5\", \"Q: 5+0=\\nA:\": \"5\", \"Q: 6-1=\\nA:\": \"5\",\n",
    "    \"Q: 7-2=\\nA:\": \"5\", \"Q: 9-4=\\nA:\": \"5\"\n",
    "}\n",
    "\n",
    "SPECIALS = [\"<pad>\", \"<bos>\", \"<eos>\"]\n",
    "vocab = sorted(set(\"\".join(PROMPTS)+ \"\".join(GOLD.values()) + \" 0123456789+-=:Q\\nA\"))\n",
    "itos = SPECIALS + vocab\n",
    "stoi = {ch:i for i,ch in enumerate(itos)}\n",
    "PAD, BOS, EOS = stoi[\"<pad>\"], stoi[\"<bos>\"], stoi[\"<eos>\"]\n",
    "V = len(itos)\n",
    "\n",
    "def encode(s:str):\n",
    "    return [BOS] + [stoi[c] for c in s] + [EOS]\n",
    "def decode(ids:List[int]):\n",
    "    s = \"\".join(itos[i] for i in ids if i>=0)\n",
    "    return s.replace(\"<bos>\",\"\").replace(\"<eos>\",\"\")\n",
    "\n",
    "# -----------------------------\n",
    "# Bigram LM (Karpathy-style)\n",
    "# -----------------------------\n",
    "class BigramLM(nn.Module):\n",
    "    def __init__(self, vocab_size:int, d_model:int=64):\n",
    "        super().__init__()\n",
    "        self.tok = nn.Embedding(vocab_size, d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "    def forward(self, idx):   # idx: (B,T)\n",
    "        h = self.tok(idx)    # (B,T,d)\n",
    "        logits = self.head(h)# (B,T,V)\n",
    "        return logits\n",
    "    def logprob_of(self, idx_in, idx_out):  # teacher-forced logprob of next-token targets\n",
    "        # idx_in, idx_out: (B,T)\n",
    "        logits = self.forward(idx_in)\n",
    "        logp = F.log_softmax(logits, dim=-1)\n",
    "        # gather logp of chosen next tokens\n",
    "        lp = logp.gather(-1, idx_out.unsqueeze(-1)).squeeze(-1)  # (B,T)\n",
    "        return lp\n",
    "\n",
    "# -----------------------------\n",
    "# Tiny Reward Model r_phi(x,y)\n",
    "# Encoder = mean of token embeddings -> linear scalar\n",
    "# -----------------------------\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, vocab_size:int, d:int=64):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, d)\n",
    "        self.head = nn.Linear(d, 1)\n",
    "    def forward(self, xy):   # xy: (B,T)\n",
    "        e = self.emb(xy)    # (B,T,d)\n",
    "        m = e.mean(dim=1)   # (B,d)\n",
    "        return self.head(m).squeeze(-1)  # (B,)\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities\n",
    "# -----------------------------\n",
    "def pad_to_batch(seqs:List[List[int]], pad_id:int=PAD):\n",
    "    T = max(len(s) for s in seqs)\n",
    "    out = []\n",
    "    for s in seqs:\n",
    "        out.append(s + [pad_id]*(T-len(s)))\n",
    "    return torch.tensor(out, dtype=torch.long)\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_completion(model:BigramLM, prompt_ids:List[int], max_new:int=10, temperature:float=1.0):\n",
    "    model.eval()\n",
    "    ids = prompt_ids[:]  # include BOS ... tokens\n",
    "    for _ in range(max_new):\n",
    "        x = torch.tensor(ids, dtype=torch.long).unsqueeze(0)  # (1,t)\n",
    "        logits = model(x)[:, -1, :] / max(1e-6, temperature)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1).item()\n",
    "        ids.append(next_id)\n",
    "        if next_id == EOS: break\n",
    "    return ids\n",
    "\n",
    "# -----------------------------\n",
    "# SFT stage (optional but recommended)\n",
    "# Train policy to imitate a few gold answers\n",
    "# -----------------------------\n",
    "def sft_train(policy:BigramLM, pairs:List[Tuple[str,str]], iters=200, lr=1e-2):\n",
    "    opt = torch.optim.AdamW(policy.parameters(), lr=lr)\n",
    "    policy.train()\n",
    "    for it in range(iters):\n",
    "        X, Y = [], []\n",
    "        for x,y in pairs:\n",
    "            sx = encode(x)\n",
    "            sy = encode(y)\n",
    "            # teacher force next-token: input is prefix, target is next tokens in y\n",
    "            # here, just concatenate x and y so LM learns to answer after prompt\n",
    "            xy = sx + sy[1:]  # share one BOS\n",
    "            x_in = xy[:-1]; x_out = xy[1:]\n",
    "            X.append(x_in); Y.append(x_out)\n",
    "        xb = pad_to_batch(X); yb = pad_to_batch(Y)\n",
    "        logp = policy.logprob_of(xb, yb)\n",
    "        loss = -logp.masked_fill(yb==PAD, 0).mean()\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        if (it+1)%100==0:\n",
    "            print(f\"[SFT] iter {it+1} loss={loss.item():.3f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Build preference data to train Reward Model\n",
    "# Use an AUTOMATIC judge: +1 if final char matches GOLD, else 0.\n",
    "# You can flip to RANDOM to illustrate noisy preferences.\n",
    "# -----------------------------\n",
    "def build_pref_dataset(policy:BigramLM, prompts:List[str], per_prompt:int=4,\n",
    "                       judge=\"AUTO\") -> List[Tuple[List[int], List[int], int]]:\n",
    "    data = []\n",
    "    for x in prompts:\n",
    "        px = encode(x)\n",
    "        cands = []\n",
    "        for _ in range(per_prompt):\n",
    "            y_ids = sample_completion(policy, px, max_new=3, temperature=1.2)\n",
    "            cands.append(y_ids)\n",
    "        # create pairwise preferences\n",
    "        for i in range(len(cands)):\n",
    "            for j in range(i+1, len(cands)):\n",
    "                yi, yj = cands[i], cands[j]\n",
    "                if judge == \"RANDOM\":\n",
    "                    pref = random.choice([0,1])   # 1 if yi preferred, else 0\n",
    "                else:\n",
    "                    # AUTO: read last visible char vs gold\n",
    "                    out_i = decode(yi)[len(x):].strip()\n",
    "                    out_j = decode(yj)[len(x):].strip()\n",
    "                    gi = int(GOLD[x]); si = 1 if out_i[:1]==str(gi) else 0\n",
    "                    sj = 1 if out_j[:1]==str(gi) else 0\n",
    "                    if si==sj:   # tie-break\n",
    "                        pref = 1 if len(out_i)>=len(out_j) else 0\n",
    "                    else:\n",
    "                        pref = 1 if si>sj else 0\n",
    "                data.append((px, yi, pref))  # store one-sided pairs by swapping as needed below\n",
    "                data.append((px, yj, 1-pref))\n",
    "    return data\n",
    "\n",
    "def train_reward_model(rm:RewardModel, pref_data, iters=400, lr=1e-3):\n",
    "    # Convert pairwise prefs into Bradley–Terry mini-batches by sampling pairs\n",
    "    opt = torch.optim.AdamW(rm.parameters(), lr=lr)\n",
    "    for it in range(iters):\n",
    "        batch = random.sample(pref_data, k=min(16, len(pref_data)))\n",
    "        # Build pairs y_w, y_l w.r.t. pref flag\n",
    "        XYw, XYl = [], []\n",
    "        for px, yseq, pref in batch:\n",
    "            # draw a second sample from same prompt to oppose\n",
    "            px2, yseq2, pref2 = random.choice([z for z in pref_data if z[0]==px])\n",
    "            # decide winner/loser using stored pref flags\n",
    "            if pref==1: (yw,yl)=(yseq, yseq2)\n",
    "            else:       (yw,yl)=(yseq2, yseq)\n",
    "            XYw.append(pad_to_batch([px + yw[ len(px): ]]))  # prompt+completion\n",
    "            XYl.append(pad_to_batch([px + yl[ len(px): ]]))\n",
    "        XYw = torch.cat(XYw, dim=0); XYl = torch.cat(XYl, dim=0)\n",
    "        sw = rm(XYw); sl = rm(XYl)\n",
    "        loss = F.binary_cross_entropy_with_logits(sw - sl, torch.ones_like(sw))\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        if (it+1)%100==0:\n",
    "            print(f\"[RM ] iter {it+1} loss={loss.item():.3f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# PPO fine-tuning against r_phi\n",
    "# critic-free PPO with batch baseline + KL to reference\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class PPOCfg:\n",
    "    max_new:int = 3\n",
    "    batch_size:int = 16\n",
    "    epochs:int = 50\n",
    "    ppo_steps:int = 4\n",
    "    lr:float = 5e-3\n",
    "    eps:float = 0.2        # PPO clip\n",
    "    beta:float = 0.05      # KL-to-reference strength\n",
    "    temperature:float = 1.0\n",
    "\n",
    "def rollout(policy:BigramLM, prompts:List[str], max_new:int, temperature:float):\n",
    "    # returns lists of (prompt_ids, full_ids, actions_ids, old_logprobs per token)\n",
    "    policy.eval()\n",
    "    records = []\n",
    "    with torch.no_grad():\n",
    "        for x in random.sample(prompts, len(prompts)):\n",
    "            px = encode(x)\n",
    "            ids = px[:]\n",
    "            old_lps = []\n",
    "            actions = []\n",
    "            for _ in range(max_new):\n",
    "                x_in = torch.tensor(ids, dtype=torch.long).unsqueeze(0)\n",
    "                logits = policy(x_in)[:, -1, :] / max(1e-6, temperature)\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                a = torch.multinomial(probs, num_samples=1).item()\n",
    "                lp = torch.log(probs[0, a]+1e-9).item()\n",
    "                actions.append(a); old_lps.append(lp)\n",
    "                ids.append(a)\n",
    "                if a==EOS: break\n",
    "            records.append((px, ids, actions, old_lps))\n",
    "    return records\n",
    "\n",
    "def ppo_train(policy:BigramLM, ref:BigramLM, rm:RewardModel, cfg:PPOCfg):\n",
    "    opt = torch.optim.AdamW(policy.parameters(), lr=cfg.lr)\n",
    "    for epoch in range(cfg.epochs):\n",
    "        # freeze a snapshot as old policy for this epoch\n",
    "        old = copy.deepcopy(policy).eval()\n",
    "        # collect rollouts\n",
    "        recs = rollout(old, PROMPTS, cfg.max_new, cfg.temperature)\n",
    "        # score rewards with reward model\n",
    "        with torch.no_grad():\n",
    "            Rs = []\n",
    "            XY = []\n",
    "            for px, ids, actions, _ in recs:\n",
    "                xy = pad_to_batch([ids])\n",
    "                XY.append(xy)\n",
    "            XY = torch.cat(XY, dim=0)\n",
    "            R = rm(XY)  # (N,)\n",
    "            Rs = R.tolist()\n",
    "        # normalize rewards (baseline)\n",
    "        meanR = sum(Rs)/len(Rs); stdR = max(1e-6, (sum((r-meanR)**2 for r in Rs)/len(Rs))**0.5)\n",
    "\n",
    "        # multiple PPO epochs over the same batch (toy)\n",
    "        for _ in range(cfg.ppo_steps):\n",
    "            L_clip, KL = 0.0, 0.0\n",
    "            n_tok = 0\n",
    "            for (px, ids, actions, old_lps), R in zip(recs, Rs):\n",
    "                # advantages: broadcast sequence-level baseline\n",
    "                A = (R - meanR)/stdR\n",
    "                # get per-token new/old/ref logprobs for taken actions\n",
    "                # build per-time-step contexts to compute logprobs\n",
    "                new_lps = []; ref_lps = []\n",
    "                cur = px[:]\n",
    "                for a in actions:\n",
    "                    x_in = torch.tensor(cur, dtype=torch.long).unsqueeze(0)\n",
    "                    lp_new = F.log_softmax(policy(x_in)[:, -1, :], dim=-1)[0, a]\n",
    "                    lp_ref = F.log_softmax(ref(x_in)[:, -1, :], dim=-1)[0, a]\n",
    "                    new_lps.append(lp_new)\n",
    "                    ref_lps.append(lp_ref)\n",
    "                    cur = cur + [a]\n",
    "                new_lps = torch.stack(new_lps); ref_lps = torch.stack(ref_lps)\n",
    "                old_lps_t = torch.tensor(old_lps)\n",
    "\n",
    "                ratio = torch.exp(new_lps - old_lps_t)\n",
    "                unclipped = ratio * A\n",
    "                clipped   = torch.clamp(ratio, 1-cfg.eps, 1+cfg.eps) * A\n",
    "                L_clip += torch.minimum(unclipped, clipped).sum()\n",
    "\n",
    "                KL += (new_lps - ref_lps).sum()  # MC estimate of KL on sampled actions\n",
    "                n_tok += len(actions)\n",
    "\n",
    "            # normalize by token count\n",
    "            L_clip = L_clip / max(1, n_tok)\n",
    "            KL = KL / max(1, n_tok)\n",
    "            loss = -L_clip + cfg.beta * KL\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "        print(f\"[PPO] epoch {epoch+1:03d} loss={loss.item():.3f} L_clip={L_clip.item():.3f} KL={KL.item():.3f} meanR={meanR:.3f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Run the pipeline\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) init policy, reference, reward model\n",
    "    policy = BigramLM(V, d_model=64)\n",
    "    ref    = copy.deepcopy(policy).eval()  # reference anchor (after SFT we reset this)\n",
    "    rm     = RewardModel(V, d=64)\n",
    "\n",
    "    # 2) (optional) SFT on a few gold exemplars\n",
    "    sft_pairs = [(x, GOLD[x]) for x in PROMPTS]\n",
    "    sft_train(policy, sft_pairs, iters=200, lr=1e-2)\n",
    "    ref = copy.deepcopy(policy).eval()  # freeze SFT as reference\n",
    "\n",
    "    # 3) Build preference data using AUTO judge (or \"RANDOM\")\n",
    "    pref_data = build_pref_dataset(policy, PROMPTS, per_prompt=4, judge=\"AUTO\")\n",
    "\n",
    "    # 4) Train reward model from preferences (Bradley–Terry)\n",
    "    train_reward_model(rm, pref_data, iters=300, lr=1e-3)\n",
    "\n",
    "    # 5) PPO fine-tune policy against r_phi with KL-to-ref\n",
    "    cfg = PPOCfg(epochs=30, ppo_steps=3, beta=0.05, eps=0.2, lr=5e-3, max_new=3, batch_size=16, temperature=1.2)\n",
    "    ppo_train(policy, ref, rm, cfg)\n",
    "\n",
    "    # 6) Inspect generations\n",
    "    for x in PROMPTS[:4]:\n",
    "        ids = sample_completion(policy, encode(x), max_new=3, temperature=0.8)\n",
    "        print(x, decode(ids)[len(x):].strip())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
